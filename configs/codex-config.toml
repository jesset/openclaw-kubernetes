# Codex CLI Configuration
# This configuration uses LiteLLM as the model provider gateway
# See: https://docs.litellm.ai/docs/simple_proxy
model = "gpt-5.2-codex"
model_provider = "github"
model_verbosity = "medium"
approval_policy = "on-failure"
sandbox_mode = "workspace-write"
model_reasoning_effort = "high"
suppress_unstable_features_warning = true
web_search = "disabled"

[features]
skills = true
unified_exec = true
shell_snapshot = true
collab = true
steer = true
collaboration_modes = true
personality = true

# Model providers (LiteLLM as Github Copilot proxy)
[model_providers.github]
name     = "Github"
base_url = "http://localhost:4000"
http_headers = { "Authorization"= "Bearer sk-dummy" }
wire_api = "responses"

